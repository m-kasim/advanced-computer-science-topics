Support Vector Machines. Support vectors. Decision boundaries. Algorithm. Kernel function.

1. ⏺ / ‹⎠› /🔼 Support-Vector Machines: [video]  https://www.youtube.com/watch?v=Lpr__X8zuE8
- [definition]: analyze data used for classification and regression analysis
- also called: support-vector networks
- type: supervised learning
- invented by Vladimir N. Vapnik and Alexey Ya. Chervonenkis in 1963
The SVM is only directly applicable for two-class tasks.

- tries to find the best line which separates both classes the farthest from each other by increasing the distance.
- more precisely, the distance between the two supporting vectors is maximized
- the larger parameters you use for the linear equation => the two support vectors get closer to teach other (and vice versa)
- [classification]: classification is the problem of identifying to which of a set of categories (sub-populations) a new observation belongs,
on the basis of a training set of data containing observations (or instances) whose category membership is known
[example]: email is spam or not spam class

- [regression analysis]: regression analysis is estimating the relationships among dependent and independent variables via continuous values

- [non-probabilistic binary linear classifier]: Given a set of training examples, each marked as belonging to one or the other of two
categories, an SVM training algorithm builds a model that assigns new examples to one category or the other

- [probabilistic classifier]: able to predict, given an observation of an input, a probability distribution over a set of classes,
rather than only outputting the most likely class that the observation should belong to

- [probability distribution]: mathematical function that provides the probabilities of occurrence of different outcomes in an experiment

- [definition]: representation of the examples as points in space, mapped so that the examples of the separate categories are
divided by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category
a support-vector machine constructs a hyperplane or set of hyperplanes in a high- or infinite-dimensional space, which can be used
for classification, regression, or other tasks like outliers detection


[how it works]:
- Looks at the [extremes] of values and draws a [decision boundary] which best separates
  both classes
- only [support vectors] are important. training examples are ignorable (because not all of them define
  the support vector)

- [data points]: n-dimensional vectors
| 65 |
| 23 |
| 7  |
| 42 |

- [hyperplane separation]: whether we can separate the data of (n)dimensional vectors with a (n-1)dimensional [hyperplane].
this is called a linear classifier.  the best hyperplane is the one that represents the largest separation, or margin, between the two classes.
it is known as the maximum-margin hyperplane and the linear classifier it defines is known as a maximum-margin classifier

- [hyperplane]: a subspace whose dimension is one less than that of its ambient space.
If a space is (3)dimensional => its hyperplanes are (2)dimensional planes

2. Support vectors.
[dot product]
3. Decision boundaries.
[classification errors]: there are 3 and they together form the SVM classification error which needs to be [minimized]
- [blue classification error]: red points classified as blue (and the distance of those points to the support vector)
- [red classification error]: blue points classified as red (and the distance of those points to the support vector)
- [margin error]: the smaller the margins between both boundaries => the bigger the error!
- [decision boundaries]: 2 / root( a^2 + b^2)
- [hyperparameter]: you can attach a hyperparameter to one of the error classifiers to increase its weight

[gradient descent]:
- from large error to small error

4. Algorithm.
-  it often happens that the sets to discriminate are not linearly separable in that space. For this reason, it was proposed[by whom?] that the original finite-dimensional space be mapped into a much higher-dimensional space, presumably making the separation easier in that space

[step 1]: start with a random hyperplane and two lines with equal orthogonal distances to it:
ax + by + c = 0
ax + by + c = -1
ax + by + c = 1

[step 2]: pick a number of epochs (repetitions)
[step 3]: pick an expanding factor between the two support vectors 0.99 (close to 1)
[step 3.5]: pick learning rate: 0.01
[step 4]: loop: pick a random point ([p] and [q])
if point [p][q] is correctly classified:
- do nothing
if point [p][q] incorrectly classified:
    if [class 1] and a*[p] + b*[q] + c > 0
    *) move the hyperplane closer to this point:
    a - [learning rate]*[p]
    b - [learning rate]*[q]
    c - [learning rate]

    or

    if [class 2] and a*[p] + b*[q] + c < 0
    *) move the hyperplane closer to this point:
    a + [learning rate]*[p]
    b + [learning rate]*[q]
    c + [learning rate]
    *) separate the lines via the expanding factor

    a * [expanding factor]
    b * [expanding factor]
    c * [expanding factor]
[step 5]: finished!

[classification error]: the distance from the support vector to the wrongly classified point

5. Kernel (trick) function:
- In 1992, Vapnik suggested a way to create nonlinear classifiers by applying the kernel trick to maximum-margin hyperplanes
- in addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the
kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.
- [kernel trick]:
Kernel methods owe their name to the use of kernel functions, which enable them to operate in a high-dimensional, implicit feature space
without ever computing the coordinates of the data in that space, but rather by simply computing the [inner products] between the images of all
pairs of data in the feature space. This operation is often computationally cheaper than the explicit computation of the coordinates
- [inner product]: a scalar function of two vectors, equal to the product of their magnitudes and the cosine of the angle between them.

- [kernel function]: keep the computational load reasonable, the mappings used by SVM schemes are designed to ensure that dot products of pairs of
input data vectors may be computed easily in terms of the variables in the original space, by defining them in terms of a kernel function
K(x, y) selected to suit the problem

- the hyperplanes in the higher-dimensional space are defined as the set of points whose dot product with a vector in that space is constant,
where such a set of vectors is an orthogonal (and thus minimal) set of vectors that defines a hyperplane

- [kernel trick]: you can "bend" a hyperplane more than once

- The vectors defining the hyperplanes can be chosen to be linear combinations with parameters of the of feature vectors

- the fact that the set of points x mapped into any hyperplane can be quite convoluted as a result, allowing much more complex discrimination
between sets that are not convex at all in the original space

- [weaknesses]:
- data with errors
- choosing the wrong kernel (choosing the kernel is an art)
- large data sets can break it
- human overwatch is required. they are powerful but not fully independent

- [kernel function example]: f(x, y) => f = x•y + x^2 * y^2
- [kernel function] is actually a [similarity function]
The effectiveness of SVM depends on the selection of kernel, the kernel's parameters, and soft margin parameter C. A common choice is a Gaussian kernel, which has a single parameter


6. Bonus:
[support vector clustering]:
When data is unlabelled, supervised learning is not possible, and an unsupervised learning approach is required, which attempts to find natural clustering of the data to groups, and then map new data to these formed groups. The support-vector clustering

[kernel machine]: used to compute a non-linearly separable functions into a higher dimension linearly separable function.

Multiclass SVM

Building binary classifiers that distinguish between one of the labels and the rest (one-versus-all) or between every pair of classes (one-versus-one). Classification of new instances for the one-versus-all case is done by a winner-takes-all strategy, in which the classifier with the highest-output function assigns the class (it is important that the output functions be calibrated to produce comparable scores). For the one-versus-one approach, classification is done by a max-wins voting strategy, in which every classifier assigns the instance to one of the two classes, then the vote for the assigned class is increased by one vote, and finally the class with the most votes determines the instance classification.\


7. Applications:
- Text and HTML categorization
- Image classification
- Voice classification (10 digits)
- Optical-character recognition

8. TODO:
MIT lecture and recitation notes
