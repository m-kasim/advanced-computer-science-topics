Neural Networks. Back propagation. Deep Neural Networks. Convolutional Neural Networks.

1. ğŸ§  Artificial Neural Networks:
- [multilayer perceptron] = deep neural network
- [â¡ï¸ input]: pixels
- [ğŸ”€ neural connections] they have [ğŸ”‹ weights] and they connect neurons. they have multipliers [float +-]
*) ( [ğŸ”˜ neuron.1] * [ğŸ”‹ weight.connection.1] ) + ( [ğŸ”˜ neuron.2] * [ğŸ”‹ weight.connection.2] ) + ... Â± [âš¡ï¸ bias] ) = ...
*) each [ğŸ”˜ neuron] has an [ğŸ“ˆ activation function] [Ïƒ] (sigma) of all all the previous [ğŸ”˜ neurons] * with their [ğŸ”‹ weights]
- [âš¡ï¸ bias]: each [ğŸ”€ neural connections] has also a [âš¡ï¸ bias], which is able to influence (once more) its [ğŸ“ˆ activation function]
  as an extra parameter

- [output]:
*) class - number [0,1] (how much is the probability it is from that class).
*) the winner is the [ğŸ”˜ neuron] with highest probability class in the last [â‡ï¸ output] layer
*) [last line - components]: in a perfect world the last layer are the components of the number/entity
*) complex components can be broken down to simpler features (edges, lines)
- [ğŸ”˜ neuron]: holds a number [0,1]
- [hidden layers]: all layers between the [â¡ï¸ input] and [â‡ï¸ output]
- [ğŸ“ˆ activation of neurons]: only some neurons get activated in each layer and their combination determines which neurons in the next layer
will get activated too. different inputs generate different patterns (which might be similar)

[learning]:
- finding the correct neural connection [ğŸ”‹ weights] and additional [âš¡ï¸ biases] which can be attached to the [ğŸ“ˆ activation functions]
- [math]: finding the minimum of a function: min (function)

[trivia]:
- neural networks detect objects mostly by using edges
- neural networks can recognize digits, but have no idea how to draw them!

[biology]:
- humans recognize images by piecing together components

[activation functions]:
*) Sigmoid: Ïƒ = 1 / 1 + e^(-x)
*) Rectified linear unit (ReLU): function returns 0 if it receives any negative input, but for any positive value  x  it returns that value back.

[data sets]:
- [numbers]: MNIST database contains 60,000 training images and 10,000 testing images

[algorithm]:
1. initialize random [ğŸ”‹ weights]
2. initialize random [âš¡ï¸ biases]

[compare prediction to training data]
----------------------------------------------------------
| neural network prediction         labeled training data  |
----------------------------------------------------------
| class | probability               class | probability    |
----------------------------------------------------------
[0]         0.23                     [0]         0.00
[1]         0.03                     [1]         0.00
[2]         0.22                     [2]         0.00
[3]         0.81                     [3]         0.93    <----------
[4]         0.53                     [4]         0.00
[5]         0.43                     [5]         0.00
[6]         0.55                     [6]         0.00
[7]         0.21                     [7]         0.00
[8]         0.02                     [8]         0.05
[9]         0.93                     [9]         0.02

[error rate]:
- [error rate]: average of all errors (cost of training)
- show previously unseen (but labeled) training data to the neural network and measure how often it is able to assign their correct labels
- [cost of training]: (probability.correct - probability.predicted)^2
the way this formula works is it sums the differences of predicted and training class probabilities of all classes.
low number => low error rate (vice versa)

2. Backpropagation:
- [definition]: algorithm for calculating the [gradient descent] effectively.
a single training example specifies how all weights must be adjusted up or down (in some proportion), as calculated by the [error/cost] function

                                                    |  3.16 |  <-- More important for adjusting the [cost/error function]
                                                    |  0.02 |
                                                    | -0.54 |
                                                    |  0.44 |
-[gradient] ( [ğŸ”‹ weights].all , [âš¡ï¸ biases].all ) = | -0.92 |  <-- How sensitive the [cost function/error] to each [ğŸ”‹ weight] and [âš¡ï¸ bias]
                                                    |  1.64 |
                                                    | -2.25 |
                                                    |  0.52 |
                                                    |  0.01 |  <-- Less important for adjusting the [cost/error function]

- [algorithm]: each neuron propagates backward the vector of the [gradient] (which is an average of the values to be modified),
thus updating the weights

3. Deep Neural Networks:

4. Convolutional Neural Networks:
- [convolution]: takes a sample of pixels and multiplies their values with a filter which represents a minimized version of a desired feature
- [pooling]: takes the average (or maximum or etc.) of a large group of neighbouring pixels and reduces it to one value

6. Gradient descent:
- [local minimum]: it's easier to find a local minimum of a function, based on the [ğŸ“ slope]
- [global maximum]: hard business!
- [gradient]: it's actually a vector and each value shows which value matters the most for adjusting a weight
- [ğŸ‘ step / ğŸ“ slope ratio]: the smaller the slope => the smaller the step (guarantees you to not overshoot and go too far)
- [gradient]: direction of the steepest ascend of a function
- [negative gradient]: direction with steepest descent

[â„¹ï¸ algorithm]:
1. compute [gradient].
2. take a step in the direction with proportion to the slope of the descent.
3. repeat
